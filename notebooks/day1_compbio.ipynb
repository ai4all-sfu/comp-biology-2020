{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day1_compbio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmpxADCbBsTe",
        "colab_type": "text"
      },
      "source": [
        "# Day 1: Introduction to RxRx19 dataset\n",
        "\n",
        "Today we will be familiarizing ourselves with the dataset structure, and performing a few operations to understand it better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf1ri6RuCRsC",
        "colab_type": "text"
      },
      "source": [
        "## 1. Loading the datasets\n",
        "\n",
        "The first step is to clone the repository with the data. The original dataset available on Kaggle has more than 305,520 images, with total size of >400 GB. Hence, we created a subset of 16,000 images and their labels.\n",
        "\n",
        "> If you are curious to know how we created this subset, you can check [here](https://github.com/ai4all-sfu/comp-biology-2020/blob/master/day0-data-preprocessing.ipynb).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAjxcgBHB8Gt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/ai4all-sfu/comp-biology-2020.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7eigUiDfpmvB",
        "colab_type": "text"
      },
      "source": [
        "To check if the files are available, we use the code below. We should see two folders: 'sample_data' and 'comp-biology-2020'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMno55AqpoxM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtSPOPNrpvLQ",
        "colab_type": "text"
      },
      "source": [
        "Let us take a look at the structure of the dataset once again.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1K8bj6FqO9mbz1hdbxK969B92PQvFxgdI)\n",
        "\n",
        "RxRx19-images.zip consists of the image dataset. As discussed on the slides, two types of cells are being considered: HRCE and Vero cells. Each folder consists of cell images taken from 26 Plates. Each plate has ~26,000 images. Each cell site is passed through five channels, so each channel has ~5000 images. \n",
        "\n",
        "Let us take a look at the metadata and embeddings file. To reduce the size of the files, we saved them in pickle format. Let's unpickle them now.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7PeuM7HwpN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import pickle\n",
        "\n",
        "embeddings = pd.read_pickle('comp-biology-2020/embeddings.pkl', compression = 'xz')\n",
        "metadata = pd.read_pickle('comp-biology-2020/metadata.pkl', compression = 'xz')\n",
        "\n",
        "#changing the index\n",
        "embeddings.set_index('site_id', inplace=True)"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0OsZ67_w17Y",
        "colab_type": "text"
      },
      "source": [
        "Let us print the head of metadata.pkl so that we can understand what it contains."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wZnQHsraw_Dj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY4W7QsWxDWK",
        "colab_type": "text"
      },
      "source": [
        "## 2. Understanding the Data\n",
        "\n",
        "* 'site_id': it refers to the cell site under consideration. Every cell site has a unique site_id. As we discussed, every cell is analyzed under 4 sites, and each site is analyzed under 5 different channels. The format of site_id is as follows: 'experiment_plate_well_site'. So each of these will have an image each for the five different channels. Recall the comparison chart between them from the slides. That image is given below for your reference.\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1MhREWlcUghzmwiMMVz8GXU_p6zWZYonu)\n",
        "\n",
        "* 'disease_condition': it refers to whether the cell is infected with the SARS-CoV-2 virus or not. In the original 'metadata.csv' there were three disease conditions: Active SARS-CoV-2 (the cell has been infected with the virus), UV Inactivated SARS-CoV-2, and Mock (mock preparations of SARS-CoV-2 on cells). We combined the disease conditions 'UV Inactivated SARS-CoV-2' and 'Mock' into one class: 'Inactive' (because they are similar), so that we can consider two distinct classes in our classification task: 'Active' and 'Inactive'.\n",
        "\n",
        "* 'treatment': it refers to the drug that is used to treat the cell from the virus, 'treatment_conc' refers to the amount of concentration that the drug is used under. For an 'inactive' cell site, the value under 'treatment' and 'treatment_conc' will be NaN. \n",
        "\n",
        "Let us print embeddings.pkl and take a look at its head."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SE43CTdUyTW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3swyZwxyYki",
        "colab_type": "text"
      },
      "source": [
        "In the above result, for every site_id we can observe 1024 feature values. These are lower-dimensional feature vectors (embeddings) for the image that provides some indication of what the image includes.\n",
        "\n",
        "In our subset we have 16,000 images in total, chosen from all the four cell subfolders (HRCE-1, HRCE-2, Vero-2, and Vero-2). Each image is of dimensions 1024 x 1024 x 1. They are grayscale images. We will not be directly handling the images in our project. Instead, we will be using the embeddings.pkl file. \n",
        "\n",
        "Let us print the shape of metadata.pkl and embeddings.pkl."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKaaMt4k0uay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Metadata : \",metadata.shape)\n",
        "print(\"Embeddings : \",embeddings.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3SlLNvi02mw",
        "colab_type": "text"
      },
      "source": [
        "As we can see above, 'metadata' has 16,000 rows for the images and 10 columns for the metadata values for each image. 'embeddings' has 16,000 rows too with 1024 columns denoting 1024 feature values for each image.\n",
        "\n",
        "Let us join 'metadata' and 'embeddings' to understand how they correlate better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9tf90tI1a6V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged = pd.merge(embeddings, metadata, on=['site_id'], how='inner')\n",
        "merged.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3RWv88X1vvQ",
        "colab_type": "text"
      },
      "source": [
        "Let us pick out the first row and take a clear look at the information that we get about each cell site."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dll7C5Tj1yYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "merged.iloc[0,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKLStFK92Jm2",
        "colab_type": "text"
      },
      "source": [
        "We will be using the merged dataframe in all of our exercises today. Let's eliminate all the other column values from 'merged' and just have our 'feature embeddings' for each image and the corresponding 'disease_condition'."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SaWQS2y3eDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feat_disease = merged.iloc[:, list(range(1025)) + [-3]]\n",
        "feat_disease"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFLmB1kK3ejS",
        "colab_type": "text"
      },
      "source": [
        "If you want to learn more about this dataset, explore the following links: \n",
        "* RxRx19: The First Morphological Imaging Dataset on SARS-CoV-2 Virus ([link](https://www.rxrx.ai/rxrx19) and [github](https://gist.github.com/bmabey/ae215f5c154cbc5c3b7e0a519e3d403b))\n",
        "* RxRx19a COVID-19 Image Embeddings ([link](https://www.kaggle.com/tunguz/rxrx19a))\n",
        "\n",
        "\n",
        "Now, we are going to understand the dataset by plotting graphs to analyze the relationships between different variables in the metadata and embeddings. Then, given an image, we are going to create feature embeddings using basic Computer Vision techniques.\n",
        "\n",
        "\n",
        "On Day 2, we are going to reduce the dimension of our 'embeddings' dataset learn about several factor models, and how to scale the dataset.\n",
        "And on day 3, we will be using 'embeddings' and parsing the values of  'disease_condition' for each of those embeddings and appending them to a labels list. We will train our model using this, and evaluate it against our test dataset. Our result will be a predicted 'disease_condition' label for a new cell image from the test dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BR_IpbnITY5d",
        "colab_type": "text"
      },
      "source": [
        "Let us plot the frequency count of disease condition 'active' in the 'feat_disease' dataframe. We use pyplot to give a title to the figure, value_counts from pandas to count the number of occurences of each category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SLs-CwPT7-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as pyplot\n",
        "fig = pyplot.figure()\n",
        "fig.suptitle('\\nFrequency distribution of active and inactive disease conditions')\n",
        "feat_disease['disease_condition'].value_counts().plot.bar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CL88L7FQVNfS",
        "colab_type": "text"
      },
      "source": [
        "### Activity 1\n",
        "\n",
        "Plot the same graph, but in the form of a 'line plot', 'scatter plot', 'pie plot', and a 'density plot'. Try using the 'groupby' function from pandas to plot the graph. Add a legend and title too.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgChcp9GWW6f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#INSERT YOUR CODE HERE "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rost4gY_Wci6",
        "colab_type": "text"
      },
      "source": [
        "### Activity 2\n",
        "\n",
        "Plot the frequency distribution of all the 26 plates in the 'merged' dataframe. Use any two plot types of your choice. The plates in the x axis should be in the increasing order.\n",
        "Try plotting the same, using plot functions from matplotlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBAK4BO9WYx9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#INSERT YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYAEpus9X_z7",
        "colab_type": "text"
      },
      "source": [
        "## Feature Extraction from an image\n",
        "\n",
        "In our slides for Day 2, we had discussed two methods for feature extraction in an image. We will now be trying them out.\n",
        "\n",
        "First, we use scikit-image which is a library containing a collection of algorithms for image processing. We will use the methods 'imread' and 'imshow' to read an image, and display it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85bpgnmMYOK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from skimage.io import imread, imshow\n",
        "\n",
        "image = imread('comp-biology-2020/supplement_images_day1/E08_s2_w1.png') \n",
        "print(image.shape)\n",
        "imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QaXbhmPn9RX",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "**1. Raw Pixel Feature Vector**\n",
        "\n",
        "The simplest way is to extract the raw pixel feature vector. The image shape here is 1024 x 1024. Hence, the number of features should be 1,048,576. We can generate this using the reshape function from NumPy where we specify the dimension of the image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSttoQV3nyGP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "aaa43c48-3c6d-40b7-9a76-ce4410e3f0c8"
      },
      "source": [
        "features = np.reshape(image, (1024*1024))\n",
        "\n",
        "features.shape, features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1048576,), array([2, 1, 1, ..., 1, 1, 1], dtype=uint8))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zcYJasror6u",
        "colab_type": "text"
      },
      "source": [
        "The shape of the feature vector is (1048576, ). This is nothing but (1024*1024, ).\n",
        "\n",
        "Now, let us look at the second method for feature extraction.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rl0OX_Y2gK2",
        "colab_type": "text"
      },
      "source": [
        "**2. Extracting Edge Features**\n",
        "\n",
        "On our slides, we had learnt that the idea behind edge detection is to extract edges as features and use that as the input for the model. Recall that an edge is a sharp change in color, or image intensity.\n",
        "\n",
        "Let us again take a look at the example of a black square in a white background that we saw in the slides:\n",
        "\n",
        "![alt text](https://drive.google.com/uc?id=1UfTCfQDgGfXZzj9rjVXSNBVY0upc--6D)\n",
        "\n",
        "The 'vertical filter' in the above image is applied onto the red box by multiplying each pixel in the red box by each pixel in the filter element-wise. Each pixel in the result is achieved in exactly the same way. Then, we sum up the pixels in the result to get our vertical score. Here in our example, we get the vertical score/sum as -4. Thus, we know the pixel in question is part of a top vertical edge because we achieve the minimum value of -4.\n",
        "\n",
        "In our example above, we have used the 'Sobel vertical filter'. In our coding exercises below, we will be using both: 'Sobel vertical filter', and 'Sobel horizontal filter'.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wapAS-AuhdV",
        "colab_type": "text"
      },
      "source": [
        "Let us first test it out on a sample image of a puppy. We read the image, and convert it to grayscale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DnAWyhTitqQ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "puppy = imread('comp-biology-2020/supplement_images_day1/pupper.jpg', as_gray=True) \n",
        "puppy.shape, imshow(puppy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu8h6PHRxPbS",
        "colab_type": "text"
      },
      "source": [
        "Instead of doing the math behind applying a filter to an image, there is an in-built function in skimage library to get the result directly after applying the Sobel vertical and horizontal filters, and display it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GdywRNO8Uwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.filters import sobel_h, sobel_v\n",
        "from skimage import feature\n",
        "\n",
        "#calculating horizontal edges using sobel kernel\n",
        "edges_sobel_horizontal = sobel_h(puppy)\n",
        "#calculating vertical edges using sobel kernel\n",
        "edges_sobel_vertical = sobel_v(puppy)\n",
        "\n",
        "imshow(edges_sobel_vertical, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQjKLVM98yy8",
        "colab_type": "text"
      },
      "source": [
        "Display edges_sobel_horizontal as well and observe the differences between the two results.\n",
        "\n",
        "Now, let us try to obtain a similar result by coding the math behind applying the filter to the image.\n",
        "\n",
        "First read the image again without converting it to grayscale, Then, assign the Sobel filter values (these are fixed values) for vertical and horizontal filters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mV72zmAaxPnw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "puppy = imread('comp-biology-2020/supplement_images_day1/pupper.jpg') \n",
        "\n",
        "#define the vertical filter\n",
        "vertical_filter = [[-1,-2,-1], [0,0,0], [1,2,1]]\n",
        "\n",
        "#define the horizontal filter\n",
        "horizontal_filter = [[-1,0,1], [-2,0,2], [-1,0,1]]\n",
        "\n",
        "#get the dimensions of the image\n",
        "n,m,d = puppy.shape"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF0kkHcuAoE3",
        "colab_type": "text"
      },
      "source": [
        "### Activity 3 \n",
        "\n",
        "Follow the following steps, and code using the method discussed in the slides.\n",
        "\n",
        "Copy the original image into edges_img. We will be inserting our edge values into it one by one. Next, loop over the pixels in our original image: create a 3x3 box which is essentially a window/matrix considered in our original image, on which we will apply the filter. Then, multiply the values in the window with the ones in the vertical filter element-wise, and obtain the vertical score. Do the same for the horizontal filter. \n",
        "\n",
        "Now, in our example while studying how to apply a filter to an image, we only detected a single horizontal/vertical edge. In order to detect the horizontal edges, vertical edges, and edges that fall somewhere in between, we can combine the vertical and horizontal scores by calculating the Euclidean distance between them, and inserting this edge score into our image. This will give us the final result which we obtained in our previous example by directly using the in-built function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztdreuUbAoPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#initialize the edges image\n",
        "edges_img = puppy.copy()\n",
        "\n",
        "#loop over all pixels in the image\n",
        "for row in range(3, n-2):\n",
        "    for col in range(3, m-2):\n",
        "        \n",
        "        #create little local 3x3 box\n",
        "        local_pixels = puppy[row-1:row+2, col-1:col+2, 0]\n",
        "        \n",
        "        #INSERT YOUR CODE HERE with the help of the comments given\n",
        "        #apply the vertical filter\n",
        "        \n",
        "        #remap the vertical score\n",
        "        \n",
        "        #apply the horizontal filter\n",
        "       \n",
        "        #remap the horizontal score\n",
        "        \n",
        "        #combine the horizontal and vertical scores into a total edge score\n",
        "        #edge_score = ...\n",
        "        \n",
        "        #insert this edge score into the edges image\n",
        "        edges_img[row, col] = [edge_score]*3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Qm4bkdVDUGt",
        "colab_type": "text"
      },
      "source": [
        "Normalize the values in the image, and display."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX1jQmI7Cg1D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#remap the values in the 0-1 range in case they went out of bounds\n",
        "edges_img = edges_img/edges_img.max()\n",
        "imshow(edges_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WB_2xasASCnG",
        "colab_type": "text"
      },
      "source": [
        "Save your image results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAfPoCHuSFIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from skimage.io import imsave\n",
        "imsave('edge_puppy.jpg', edges_img) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Keostwouyxp",
        "colab_type": "text"
      },
      "source": [
        "### Activity 4\n",
        "\n",
        "1. Append the set of cell images given in the directory 'test_images_day1' to a list, and display them using subplot from matplotlib. Extract the raw pixel feature vector for these images. \n",
        "\n",
        "The *os* and *os.path* modules include functions to interact with the file system. We will be using some of them below to retrieve the image files from 'test_images_day1' directory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwMPDfvrna4d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "\n",
        "mypath='comp-biology-2020/test_images_day1'\n",
        "\n",
        "#listdir will retrieve the list of files and directories in the current directory, isfile will check whether the specified path is an existing regular file or not\n",
        "#join will join the path components 'mypath' and 'f'.\n",
        "files = [f for f in listdir(mypath) if isfile(join(mypath,f))]\n",
        "images = np.empty(len(files), dtype=object)\n",
        "for n in range(0, len(files)):\n",
        "  #INSERT YOUR CODE HERE for reading each image and appending to the images list.\n",
        "\n",
        "#INSERT YOUR CODE HERE to display images using subplot."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vli86m5fvCks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Extract the raw pixel feature vectors for the list of test images\n",
        "#INSERT YOUR CODE HERE\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HvOa8XbtlY_",
        "colab_type": "text"
      },
      "source": [
        "2. Think about how we can transform this feature vector back into the image. Try it out if you have any ideas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U13njbcStota",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#INSERT YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlQWpe8tXpbl",
        "colab_type": "text"
      },
      "source": [
        "### Activity 5 (Advanced)\n",
        "Use the edge detection method we just discussed, and extract the features for the same set of images from Activity 4. Use the inbuilt method for Sobel filter in skimage, and then the detailed math method. \n",
        "\n",
        "Hint: Use the same list 'images' that you used in the previous activity, and first display the results with the horizontal Sobel filter applied using subplot (simply reuse your code from the previous activity), and then display the results with the vertical Sobel filter applied.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5NoL7diXoLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#INSERT YOUR CODE HERE\n",
        "#initialize the edges image\n",
        "edges_images = images.copy()\n",
        "vertical_filter = [[-1,-2,-1], [0,0,0], [1,2,1]]\n",
        "horizontal_filter = [[-1,0,1], [-2,0,2], [-1,0,1]]\n",
        "\n",
        "#loop over all pixels in the images\n",
        "for img in edges_images:\n",
        "  n,m = img.shape\n",
        "  for row in range(3, n-2):\n",
        "      for col in range(3, m-2):\n",
        "          local_pixels = img[row-1:row+2, col-1:col+2]\n",
        "          vertical_transformed_pixels = vertical_filter*local_pixels\n",
        "          vertical_score = vertical_transformed_pixels.sum()/4\n",
        "          horizontal_transformed_pixels = horizontal_filter*local_pixels\n",
        "          horizontal_score = horizontal_transformed_pixels.sum()/4      \n",
        "          edge_score = (vertical_score**2 + horizontal_score**2)**.5\n",
        "          \n",
        "          img[row, col] = edge_score*3\n",
        "  \n",
        "#remap the values in the 0-1 range in case they went out of bounds\n",
        "  img = img/img.max()\n",
        "\n",
        "fig=plt.figure(figsize=(30, 30))\n",
        "columns = 5\n",
        "rows = 1\n",
        "for i in range(1, columns*rows +1):\n",
        "    fig.add_subplot(rows, columns, i)\n",
        "    plt.imshow(edges_images[i-1], cmap = 'gray')\n",
        "fig.tight_layout(pad=3.0)    \n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhbQITmtX2qh",
        "colab_type": "text"
      },
      "source": [
        "### Activity 6 (Advanced)\n",
        "Recall the explanation of Prewitt Filters, and Canny filters from the slides. Implement them using the inbuilt function in skimage, and we can discuss the results during the office hours. You can either use just the first image from test_images_day1, or all the images concatenated into a list (use 'images' from Activity 4)\n",
        "\n",
        "Note: For each activity, save your image results using imsave, to display during the next session.\n",
        "\n",
        "Feature extraction will not be used in the following days, this is just to give an introduction to image processing using basic Computer Vision techniques."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shyD6fBnp5cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#INSERT YOUR CODE HERE"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}