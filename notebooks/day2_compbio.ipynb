{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day2_compbio.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MTObEK2-8HTa",
        "colab_type": "text"
      },
      "source": [
        "# Day 2: Factor Models\n",
        "\n",
        "Yesterday we explored the embedding file and metadata.  \n",
        "\n",
        "Today, we are going to work with factor models!  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOtHueko8LzJ",
        "colab_type": "text"
      },
      "source": [
        "## 1. Loading the datasets \n",
        "\n",
        "The first step is to clone the repo with the data. The original dataset available at [Kaggle](https://www.kaggle.com/tunguz/rxrx19a?select=embeddings.csv) has more than 3 GB and 305520 images. So we create a subset with only 15000 images. \n",
        "\n",
        "If you are curious to know how we created this subset, you can check our code in the [github repositoty](https://github.com/ai4all-sfu/comp-biology-2020/blob/master/day0-data-preprocessing.ipynb). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bq0n9qy7VbVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! git clone https://github.com/ai4all-sfu/comp-biology-2020.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiYjiCabYeX6",
        "colab_type": "text"
      },
      "source": [
        "[link text](https://)To check if the files are available, we use the code below. We should see two folders: 'sample_data' and 'comp-biology-2020'. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5QA6wDH_ZPq5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfKI761kZm1M",
        "colab_type": "text"
      },
      "source": [
        "## 2. Analysis\n",
        "\n",
        "The data now is read to be used, so we can start our analysis! \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzdUH9TNXHvR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the libraries \n",
        "#Hint: You only need to do this once in your code\n",
        "import pandas as pd \n",
        "import numpy as np \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JM7pJg3W62NK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading Files we cloned from github \n",
        "embeddings = pd.read_pickle('comp-biology-2020/embeddings.pkl', compression = 'xz')\n",
        "metadata = pd.read_pickle('comp-biology-2020/metadata.pkl', compression = 'xz')\n",
        "\n",
        "#changing the index\n",
        "embeddings.set_index('site_id', inplace=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KQvozbaZ3LG",
        "colab_type": "text"
      },
      "source": [
        "With these libraries and the files load, the data is read for our analysis. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_r0R8018lk_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Checking how big are the datasets: \n",
        "print('Dimensions embeddings data: ',embeddings.shape)\n",
        "print('Dimensions metadata data  : ',metadata.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeupO_oIac2p",
        "colab_type": "text"
      },
      "source": [
        "We can also check the format of the metadata file:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAOsgaIRYd5j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metadata.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP0cdonSDGah",
        "colab_type": "text"
      },
      "source": [
        "The last step in the pre-processing is to standardize the dataset. This is a crucial step to avoid that some features have a more considerable influence in the final results only because their scale is larger than other features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWOPA-tlDBdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "#standardize the datasetâ€™s features onto unit scale (mean = 0 and variance = 1)\n",
        "x = StandardScaler().fit_transform(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHPCv210annF",
        "colab_type": "text"
      },
      "source": [
        "The metadata has the label of interest in the column 'disease_condition', and it will be more explored in the Day 3. \n",
        "\n",
        "Today, we are going to work on a very common challenge in Machine Learning: dimensionality reduction. \n",
        "\n",
        "The embeddings file has 1025 columns. Such large number of columns increase the complexity of the classification models, cause overfitting, and be very time-consuming. \n",
        "\n",
        "In the following analysis, we will explore three different methods to reduce the dimensionality of these datasets, known as *factor models*. \n",
        "\n",
        "To see more details about each method, check the slides. \n",
        "\n",
        "# Principal Components Analysis (PCA)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLqqr74YBtvH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the PCA library  \n",
        "from sklearn.decomposition import PCA \n",
        "#different optimizers \n",
        "solver = ['auto', 'full', 'arpack', 'randomized']\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDm2BDmmB_6M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Model Definition\n",
        "#Number of features\n",
        "k1 = 50 \n",
        "#Model definition\n",
        "pca = PCA(n_components=k1, svd_solver =solver[0]) \n",
        "#Model Fitting\n",
        "pca.fit(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lRznmMSGXmi",
        "colab_type": "text"
      },
      "source": [
        "How good are the latent features?\n",
        "\n",
        "When performing a PCA, a simple way to verify if the latent features are good is to check the variance explained by the principal components.\n",
        "Below, we will construct a plot to see the value by each component individually and a cumulative value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QV4RKZqfJxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cumulative value: \n",
        "print('Explained Variance:', sum(pca.explained_variance_ratio_)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMxK8fupJ7kC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(np.arange(1, pca.n_components_ + 1),\n",
        "         pca.explained_variance_ratio_, '+', linewidth=2)\n",
        "ax.set_ylabel('PCA explained variance ratio')\n",
        "ax.set_xlabel('Number of Components')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYjUxJyhK1mt",
        "colab_type": "text"
      },
      "source": [
        "For k1=50 and solver = 'auto', the PCA results seems good. So we will move foward with these latent features. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vx-nK-aABWf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#principal components has a lower dimension and represents our latent variables\n",
        "principalComponents = pca.transform(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOmS_4p_9M9X",
        "colab_type": "text"
      },
      "source": [
        "The last step is transform the latent features to a scale between 0 and 1. By doing this, we can improve the performance of the classification models. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfZDyctj9a66",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the function to make the scale\n",
        "scale01 = MinMaxScaler()\n",
        "#Scaling the principal componentes \n",
        "principalComponents = scale01.fit_transform(principalComponents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6CIFbTULJn2",
        "colab_type": "text"
      },
      "source": [
        "### Activity 1: \n",
        "\n",
        "Explore other combinations of K and svd_solvers. Can you find other promissing sets of latent features?\n",
        "\n",
        "Save your best result to be used in the classification model tomorrow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xd5rFELuKx-v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Model definition and fit: \n",
        "k = "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrARJLX8LiHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Evaluation: Plot and percentage of variance explained \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxyD9qU5L5IQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mypca = pca.fit_transform(x) #your new pca\n",
        "mypca = scale01.fit_transform(mypca) #scale between 0 and 1\n",
        "print('My k is ', principalComponents.shape[1])\n",
        "np.savez_compressed('mypca.npz',mypca)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4hO_onFMwaY",
        "colab_type": "text"
      },
      "source": [
        "After saving, click on the folders on the left side, and you should see your 'mypca.npz' file. If not, click in 'refresh'.\n",
        "\n",
        "Download your file by clicking on the three dots on the right side of your file's name. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLTdsPk6eM1m",
        "colab_type": "text"
      },
      "source": [
        "# Matrix Factorization\n",
        "\n",
        "This factor model will decompose the original dataset in two smaller matrices. \n",
        "\n",
        "For more information, check out the slides! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAa3edL78sqa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "#The input data should be between 0 and 1\n",
        "x01 = scale01.fit_transform(x)\n",
        "\n",
        "#Size of the latent variables\n",
        "k2 = 60\n",
        "#model definition \n",
        "nmf = NMF(n_components=k2, random_state=0, init = 'nndsvda') \n",
        "#model fitting\n",
        "nmf.fit(x01)\n",
        "#nmf_features has the latent variables with dimension k2\n",
        "nmf_features = nmf.transform(x01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cDPS3WMOxaE",
        "colab_type": "text"
      },
      "source": [
        "To evaluate the matrix factorization, we measure how well it reconstruct the original data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHDc7irYeIza",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Original Variables:\\n', x01[0:4, 0:4])\n",
        "reconstruction = np.dot(nmf_features,nmf.components_) \n",
        "print('New  Variables:\\n', reconstruction[0:4, 0:4])\n",
        "\n",
        "print('Average Error: ', nmf.reconstruction_err_/(x01.shape[0]*x01.shape[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKOYreAP_kIN",
        "colab_type": "text"
      },
      "source": [
        "The reconstructed matrix looks very close the original variables, and the average error is very low. So we will keep the latent features of this factor model. Before saving, we are going to transform them to be between 0 and 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LEhzJhaG_WU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nmf_features = scale01.fit_transform(nmf_features) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7ZPhJkXP2g3",
        "colab_type": "text"
      },
      "source": [
        "### Activity 2: \n",
        "\n",
        "Choose a new value of k2. Run the Matrix Factorization again and save your output for tomorrow's lesson. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeZmLSacP10c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#model definition and fitting\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZtFKbayQGrW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#evaluation\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzHx1l9Tu66A",
        "colab_type": "text"
      },
      "source": [
        "Why do you think this Matrix Factorization will improve our results?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y5amVcPUQKzt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mymf = nmf.transform(x01)\n",
        "mymf = scale01.fit_transform(mymf) \n",
        "print('My k is ', mymf.shape[1])\n",
        "np.savez_compressed('mymf.npz',mymf)\n",
        "\n",
        "#Donwload the file as previously explained."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgxUGFMxePt3",
        "colab_type": "text"
      },
      "source": [
        "# Autoencoder \n",
        "\n",
        "This is the last model to reduce the dimensionality we are going to explore. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIpNbcIOPEkM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior() \n",
        "\n",
        "#Don't worry about this class object, we are only going to use it :) \n",
        "#Reference: https://rubikscode.net/2018/11/26/3-ways-to-implement-autoencoders-with-tensorflow-and-python/\n",
        "class Autoencoder(object):\n",
        "    def __init__(self, inout_dim, encoded_dim):\n",
        "        learning_rate = 0.1 \n",
        "        \n",
        "        # Weights and biases\n",
        "        hiddel_layer_weights = tf.Variable(tf.random_normal([inout_dim, encoded_dim]))\n",
        "        hiddel_layer_biases = tf.Variable(tf.random_normal([encoded_dim]))\n",
        "        output_layer_weights = tf.Variable(tf.random_normal([encoded_dim, inout_dim]))\n",
        "        output_layer_biases = tf.Variable(tf.random_normal([inout_dim]))\n",
        "        \n",
        "        # Neural network\n",
        "        self._input_layer = tf.placeholder('float', [None, inout_dim])\n",
        "        self._hidden_layer = tf.nn.relu(tf.add(tf.matmul(self._input_layer, hiddel_layer_weights), hiddel_layer_biases))\n",
        "        self._output_layer = tf.matmul(self._hidden_layer, output_layer_weights) + output_layer_biases\n",
        "        self._real_output = tf.placeholder('float', [None, inout_dim])\n",
        "        \n",
        "        self._meansq = tf.reduce_mean(tf.square(self._output_layer - self._real_output))\n",
        "        self._optimizer = tf.train.AdagradOptimizer(learning_rate).minimize(self._meansq)\n",
        "        self._training = tf.global_variables_initializer()\n",
        "        self._session = tf.Session()\n",
        "        \n",
        "    def train(self, input_train, input_test, batch_size, epochs):\n",
        "        self._session.run(self._training)\n",
        "        \n",
        "        for epoch in range(epochs):\n",
        "            epoch_loss = 0\n",
        "            for i in range(int(input_train.shape[0]/batch_size)):\n",
        "                epoch_input = input_train[ i * batch_size : (i + 1) * batch_size ]\n",
        "                _, c = self._session.run([self._optimizer, self._meansq], feed_dict={self._input_layer: epoch_input, self._real_output: epoch_input})\n",
        "                epoch_loss += c\n",
        "                print('Epoch', epoch, '/', epochs, 'loss:',epoch_loss)\n",
        "        \n",
        "    def getEncoded(self, item):\n",
        "        encoded_ = self._session.run(self._hidden_layer, feed_dict={self._input_layer:[item]})\n",
        "        return encoded_\n",
        "    \n",
        "    def getDecoded(self, item):\n",
        "        decoded_ = self._session.run(self._output_layer, feed_dict={self._input_layer:[item]})\n",
        "        return decoded_\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0LjBlkpTG4g",
        "colab_type": "text"
      },
      "source": [
        "For this factor model, we need to split the dataset into 2 parts: training and testing set. (This part takes a couple of minutes to run)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4fjerE5bTFEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test = train_test_split(x,test_size=0.33, random_state=42)\n",
        "\n",
        "#Model definition\n",
        "autoencodertf = Autoencoder(x.shape[1], 32)\n",
        "\n",
        "#Model Fitting\n",
        "autoencodertf.train(x_train, x_test, 100, 50)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YhlLkfMpawtW",
        "colab_type": "text"
      },
      "source": [
        "To evaluate the autoencoder, we are going to check how well it reconstruct the original dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRD91Yi0Sc5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "testing_error = []\n",
        "training_error = []\n",
        "\n",
        "for i in range(len(x_test)):\n",
        "  testing_error.append(mean_squared_error(x_test[i], autoencodertf.getDecoded(x_test[i]).reshape(-1,1)))\n",
        "\n",
        "for i in range(len(x_train)):\n",
        "  training_error.append(mean_squared_error(x_train[i],autoencodertf.getDecoded(x_train[i]).reshape(-1,1))) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJrX8uDVYUft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Mean Squared Error on Testing set',np.mean(testing_error))\n",
        "print('Mean Squared Error on Training set',np.mean(training_error))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGj5xf45aQcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "histogram_data = plt.hist([testing_error, training_error], bins=50, color=['b','r'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qENYzdQxa0_r",
        "colab_type": "text"
      },
      "source": [
        "The error in the training and testing set looks similar and low, so there are evidence that our autoencoder is doing a good job. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wK-C1ASbDoF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creating the Latent Variables\n",
        "autoenconderlv = []\n",
        "\n",
        "for i in range(len(x)):\n",
        "  autoenconderlv.append( autoencodertf.getEncoded(x[i])[0])\n",
        "\n",
        "autoenconderlv = np.matrix(autoenconderlv)\n",
        "print('Autoencoder latent variables shape: ', autoenconderlv.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IA8-FFErhzU",
        "colab_type": "text"
      },
      "source": [
        "## Activity 3 (advanced level) \n",
        "\n",
        "Can you think of ways to improve the autoencoder? \n",
        "\n",
        "The Mean Squared Error was in the previous example, but could we reduce it even more? \n",
        "\n",
        "Play around with the parameters and see if you can find a better set of latent variables. Start by exploring the encoded_dim and learning_rate parameters. Then, you can also try different [optimizers](https://https://www.tensorflow.org/api_docs/python/tf/compat/v1/train) and [activate functions](https://https://www.tensorflow.org/api_docs/python/tf/nn).  \n",
        "\n",
        "Save your best result for tomorrow's class! \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJwZR5xMbO9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#ADD CODE HERE \n",
        "\n",
        "#np.savez_compressed('MyAmazingAutoencoder.npz',MyAmazingAutoencoder)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}